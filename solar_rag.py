import time
import os
import base64
import uuid
import tempfile
from typing import Dict, List, Any, Optional
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader

from langchain_upstage import ChatUpstage
from langchain_core.messages import HumanMessage, SystemMessage

from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from dotenv import load_dotenv
import streamlit as st
from langsmith import Client
import os

# .env íŒŒì¼ì—ì„œ upstage key ë°›ì•„ì˜¤ê¸°
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("UPSTAGE_API_KEY")

# LangSmith í‚¤
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")

# LangSmith ê¸°ë³¸ íŠ¸ë ˆì´ì‹± ì„¤ì •(ìµœì†Œ)
os.environ["LANGCHAIN_TRACING_V2"] = "true"           # íŠ¸ë ˆì´ì‹± ON
os.environ["LANGCHAIN_PROJECT"]     = "solar-rag"     # í”„ë¡œì íŠ¸ëª…(ì—†ìœ¼ë©´ default)
os.environ["LANGCHAIN_ENDPOINT"]    = "https://api.smith.langchain.com"  # US ê¸°ë³¸

client = Client()  # envë¡œë¶€í„° ìë™ ì„¤ì •
# # í”„ë¡œì íŠ¸ ëª©ë¡ì´ ë³´ì´ë©´ ì¸ì¦ OK
projects = list(client.list_projects())[:3]
print("LangSmith projects (sample):", [p.name for p in projects] or "(none)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

# ì„¸ì…˜ ID ì„¤ì •
session_id = st.session_state.id
client = None

# ì±„íŒ… ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
def reset_chat() -> None:
    """ë‚˜ëˆ´ë˜ ëŒ€í™”ì™€ ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜
    """
    st.session_state.messages = []
    st.session_state.context = None

# ì½ì–´ì˜¨ PDF ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
def display_pdf(file) -> None:
    """PDF íŒŒì¼ì„ ë°›ì•„ì™€ì„œ ë””ìŠ¤í”Œë ˆì´ í•´ì£¼ëŠ” í•¨ìˆ˜
    """
    st.markdown ("### PDF Preview")
    base64_pdf = base64.b64encode(file.read()).decode("utf-8")
    pdf_display = f"""<iframe src="data:application/pdf;base64,{base64_pdf}" width="400" height="100%" type="application/pdf" style="height:100vh; width:100%"></iframe>"""
    st.markdown(pdf_display, unsafe_allow_html=True)

with st.sidebar:
    st.header(f"Add your documents!")
    uploaded_file = st.file_uploader("Choose your `.pdf`file",type="pdf")

    if uploaded_file:
        print(uploaded_file)
        try:
            file_key = f"{session_id}-{uploaded_file.name}"

            with tempfile.TemporaryDirectory() as temp_dir:
                file_path = os.path.join(temp_dir,uploaded_file.name)
                print("file path:", file_path)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())

                st.write("Indexing your document ...")

                # ìœ„ì—ì„œ key ì™€ temp_dir ìƒê²¼ëŠ”ì§€ ì²´í¬
                if file_key not in st.session_state.get('file_cache', {}):
                    if os.path.exists(temp_dir):
                        print("temp_dir:",temp_dir)
                        loader = PyPDFLoader(file_path)
                    else:
                        st.error('Colud not find the file you uploaded, please check again ...')
                        st.stop()
                
                    pages = loader.load_and_split()

                    vectorstore = FAISS.from_documents(pages, UpstageEmbeddings(model="solar-embedding-1-large"))

                    retriever = vectorstore.as_retriever(k=2)

                    llm = ChatUpstage()

                    contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""

                    contextualize_q_prompt = ChatPromptTemplate.from_messages(
                        [
                            ("system", contextualize_q_system_prompt),
                            MessagesPlaceholder("chat_history"),
                            ("human", "{input}"),
                        ]
                    )
                    
                    # ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
                    history_aware_retriever = create_history_aware_retriever(
                        llm, retriever, contextualize_q_prompt
                    )

                    qa_system_prompt = """ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ë‹µë³€ì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
                    ## ë‹µë³€ ì˜ˆì‹œ
                    ğŸ“ë‹µë³€ ë‚´ìš©:
                    ğŸ“ì¦ê±°:
                    {context}"""

                    qa_prompt = ChatPromptTemplate.from_messages(
                        [
                            ("system", qa_system_prompt),
                            MessagesPlaceholder("chat_history"),
                            ("human", "{input}"),
                        ]
                    )
                    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
                    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
                    
                    st.success("Ready to Chat!")
                    display_pdf(uploaded_file)
        except Exception as e:
            st.error(f"An error occuered : {e}")
            st.stop()


# ì›¹ì‚¬ì´íŠ¸ ì œëª© ì‘ì„±
st.title("Solar LLM Chatbot")

# ë©”ì„¸ì§€ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ë©”ì„¸ì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])

# ê¸°ë¡í•˜ëŠ” ëŒ€í™”ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •
MAX_MESSAGES_BEFORE_DELETION = 8

# ìœ ì €ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!"):
    # ì´ì „ ëŒ€í™”ì˜ ê¸¸ì´ í™•ì¸
    if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
        del st.session_state.messages[0]
        del st.session_state.messages[0]

    st.session_state.messages.append(
        {"role": "user","content": prompt}
    )
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ì˜ ë‹µë³€ì„ ë°›ì•„ì„œ session stateì— ì €ì¥í•˜ê³ , ë³´ì—¬ë„ ì¤˜ì•¼í•¨
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_reponse =""

        result = rag_chain.invoke(
            {'input':prompt, 'chat_history': st.session_state.messages}
        )
        # print(result)
        with st.expander("ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ"):
            st.write(result['context'])

        for chunk in result['answer'].split(" "):
            full_reponse += chunk + " "
            message_placeholder.markdown(full_reponse)
    
    st.session_state.messages.append(
        {"role": "assistant","content": full_reponse})